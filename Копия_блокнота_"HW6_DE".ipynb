{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"HW6_DE\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryMart99/COVID-19/blob/master/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22HW6_DE%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RumFnRay9oEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28823f1d-5c8a-4248-8b18-b73d3ea041a8"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u275-b01-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68_AeFo49pEU"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftRUvQn89-yC"
      },
      "source": [
        "id='1f_9EbnywCj35EBUA32sueigxBjBJwALr'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('War and Peace by Leo Tolstoy (ru).txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpR8DQxm-IRN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6qPAe12-Q19"
      },
      "source": [
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E-RdW9y-U1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "6f0aef73-ad67-47aa-9f50-beb88f0622c6"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4598314eaad6:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd727dcd198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LU3RGy-Yag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441fa544-5ee5-4368-f908-d465887e07be"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-13 20:44:44--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 35.153.20.238, 35.170.115.131, 52.55.159.231, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|35.153.20.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  13.9MB/s    in 0.9s    \n",
            "\n",
            "2020-12-13 20:44:45 (13.9 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgte5waWKrpd"
      },
      "source": [
        "Подсчитай кол-во слов в документе \"War and Peace by Leo Tolstoy (ru).txt\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7AYXXsTKqjm"
      },
      "source": [
        "textFile = spark.read.text(\"War and Peace by Leo Tolstoy (ru).txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9zIPkFfFhju",
        "outputId": "68eba204-c434-4b68-a736-323b60d30be5"
      },
      "source": [
        "textFile.count() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31931"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSQ0kB2pFkvy",
        "outputId": "f15018fb-875e-4458-bd36-219b3911a8cf"
      },
      "source": [
        "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(sum(col(\"numWords\"))).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sum(numWords)=493952)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgmIgCzLJTo"
      },
      "source": [
        "id='13yfAoONwq4rS5XrTv3IrcqcFcdgfvK9V'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-train.txt')\n",
        "\n",
        "id='1VE_9x0LQvOJpHXbXp_RMPl3Q4wRUuOok'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSU-TTUvNaON"
      },
      "source": [
        "Необходимо обучить модель используя Spark MLlib (модель на ваш выбор, например Decision Tree) и получить accuracy.\n",
        "Подробнее тут: https://spark.apache.org/docs/latest/ml-classification-regression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGetbOihmw8l",
        "outputId": "b4352497-8fbf-4791-f716-7ce503599d44"
      },
      "source": [
        "train = spark.read.format(\"libsvm\").load(\"mnist-digits-train.txt\")\r\n",
        "test= spark.read.format(\"libsvm\").load(\"mnist-digits-test.txt\")\r\n",
        "\r\n",
        "llist = train.collect()\r\n",
        "print(llist[0])\r\n",
        "print(len(llist))\r\n",
        "\r\n",
        "tlist = test.collect()\r\n",
        "print(tlist[0])\r\n",
        "print(len(tlist))\r\n",
        "\r\n",
        "#data = training.union(testing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row(label=5.0, features=SparseVector(780, {152: 3.0, 153: 18.0, 154: 18.0, 155: 18.0, 156: 126.0, 157: 136.0, 158: 175.0, 159: 26.0, 160: 166.0, 161: 255.0, 162: 247.0, 163: 127.0, 176: 30.0, 177: 36.0, 178: 94.0, 179: 154.0, 180: 170.0, 181: 253.0, 182: 253.0, 183: 253.0, 184: 253.0, 185: 253.0, 186: 225.0, 187: 172.0, 188: 253.0, 189: 242.0, 190: 195.0, 191: 64.0, 203: 49.0, 204: 238.0, 205: 253.0, 206: 253.0, 207: 253.0, 208: 253.0, 209: 253.0, 210: 253.0, 211: 253.0, 212: 253.0, 213: 251.0, 214: 93.0, 215: 82.0, 216: 82.0, 217: 56.0, 218: 39.0, 231: 18.0, 232: 219.0, 233: 253.0, 234: 253.0, 235: 253.0, 236: 253.0, 237: 253.0, 238: 198.0, 239: 182.0, 240: 247.0, 241: 241.0, 260: 80.0, 261: 156.0, 262: 107.0, 263: 253.0, 264: 253.0, 265: 205.0, 266: 11.0, 268: 43.0, 269: 154.0, 289: 14.0, 290: 1.0, 291: 154.0, 292: 253.0, 293: 90.0, 319: 139.0, 320: 253.0, 321: 190.0, 322: 2.0, 347: 11.0, 348: 190.0, 349: 253.0, 350: 70.0, 376: 35.0, 377: 241.0, 378: 225.0, 379: 160.0, 380: 108.0, 381: 1.0, 405: 81.0, 406: 240.0, 407: 253.0, 408: 253.0, 409: 119.0, 410: 25.0, 434: 45.0, 435: 186.0, 436: 253.0, 437: 253.0, 438: 150.0, 439: 27.0, 463: 16.0, 464: 93.0, 465: 252.0, 466: 253.0, 467: 187.0, 493: 249.0, 494: 253.0, 495: 249.0, 496: 64.0, 518: 46.0, 519: 130.0, 520: 183.0, 521: 253.0, 522: 253.0, 523: 207.0, 524: 2.0, 544: 39.0, 545: 148.0, 546: 229.0, 547: 253.0, 548: 253.0, 549: 253.0, 550: 250.0, 551: 182.0, 570: 24.0, 571: 114.0, 572: 221.0, 573: 253.0, 574: 253.0, 575: 253.0, 576: 253.0, 577: 201.0, 578: 78.0, 596: 23.0, 597: 66.0, 598: 213.0, 599: 253.0, 600: 253.0, 601: 253.0, 602: 253.0, 603: 198.0, 604: 81.0, 605: 2.0, 622: 18.0, 623: 171.0, 624: 219.0, 625: 253.0, 626: 253.0, 627: 253.0, 628: 253.0, 629: 195.0, 630: 80.0, 631: 9.0, 648: 55.0, 649: 172.0, 650: 226.0, 651: 253.0, 652: 253.0, 653: 253.0, 654: 253.0, 655: 244.0, 656: 133.0, 657: 11.0, 676: 136.0, 677: 253.0, 678: 253.0, 679: 253.0, 680: 212.0, 681: 135.0, 682: 132.0, 683: 16.0}))\n",
            "60000\n",
            "Row(label=7.0, features=SparseVector(778, {202: 84.0, 203: 185.0, 204: 159.0, 205: 151.0, 206: 60.0, 207: 36.0, 230: 222.0, 231: 254.0, 232: 254.0, 233: 254.0, 234: 254.0, 235: 241.0, 236: 198.0, 237: 198.0, 238: 198.0, 239: 198.0, 240: 198.0, 241: 198.0, 242: 198.0, 243: 198.0, 244: 170.0, 245: 52.0, 258: 67.0, 259: 114.0, 260: 72.0, 261: 114.0, 262: 163.0, 263: 227.0, 264: 254.0, 265: 225.0, 266: 254.0, 267: 254.0, 268: 254.0, 269: 250.0, 270: 229.0, 271: 254.0, 272: 254.0, 273: 140.0, 291: 17.0, 292: 66.0, 293: 14.0, 294: 67.0, 295: 67.0, 296: 67.0, 297: 59.0, 298: 21.0, 299: 236.0, 300: 254.0, 301: 106.0, 326: 83.0, 327: 253.0, 328: 209.0, 329: 18.0, 353: 22.0, 354: 233.0, 355: 255.0, 356: 83.0, 381: 129.0, 382: 254.0, 383: 238.0, 384: 44.0, 408: 59.0, 409: 249.0, 410: 254.0, 411: 62.0, 436: 133.0, 437: 254.0, 438: 187.0, 439: 5.0, 463: 9.0, 464: 205.0, 465: 248.0, 466: 58.0, 491: 126.0, 492: 254.0, 493: 182.0, 518: 75.0, 519: 251.0, 520: 240.0, 521: 57.0, 545: 19.0, 546: 221.0, 547: 254.0, 548: 166.0, 572: 3.0, 573: 203.0, 574: 254.0, 575: 219.0, 576: 35.0, 600: 38.0, 601: 254.0, 602: 254.0, 603: 77.0, 627: 31.0, 628: 224.0, 629: 254.0, 630: 115.0, 631: 1.0, 655: 133.0, 656: 254.0, 657: 254.0, 658: 52.0, 682: 61.0, 683: 242.0, 684: 254.0, 685: 254.0, 686: 52.0, 710: 121.0, 711: 254.0, 712: 254.0, 713: 219.0, 714: 40.0, 738: 121.0, 739: 254.0, 740: 207.0, 741: 18.0}))\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FalYk9aWz0-j",
        "outputId": "c18ae432-150c-436b-ce08-cd57836d3ade"
      },
      "source": [
        "train.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  5.0|(780,[152,153,154...|\n",
            "|  0.0|(780,[127,128,129...|\n",
            "|  4.0|(780,[160,161,162...|\n",
            "|  1.0|(780,[158,159,160...|\n",
            "|  9.0|(780,[208,209,210...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "806oUCPf2ykd",
        "outputId": "700c9b48-fab2-4863-bded-ee1f5179bfd6"
      },
      "source": [
        "test.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  7.0|(778,[202,203,204...|\n",
            "|  2.0|(778,[94,95,96,97...|\n",
            "|  1.0|(778,[128,129,130...|\n",
            "|  0.0|(778,[124,125,126...|\n",
            "|  4.0|(778,[150,151,159...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTc8AVOLiJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "9b5b7237-c2c2-46d6-c27f-ad51bebc7868"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression, OneVsRest\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "\r\n",
        "# load data file.\r\n",
        "#inputData = spark.read.format(\"libsvm\") \\.load(\"data/mllib/sample_multiclass_classification_data.txt\")\r\n",
        "\r\n",
        "# generate the train/test split.\r\n",
        "(train, test) = training.randomSplit([0.8, 0.2])\r\n",
        "testing.show()\r\n",
        "\r\n",
        "# instantiate the base classifier.\r\n",
        "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\r\n",
        "\r\n",
        "# instantiate the One Vs Rest Classifier.\r\n",
        "ovr = OneVsRest(classifier=lr)\r\n",
        "\r\n",
        "# train the multiclass model.\r\n",
        "ovrModel = ovr.fit(train)\r\n",
        "\r\n",
        "# score the model on test data.\r\n",
        "predictions = ovrModel.transform(test)\r\n",
        "\r\n",
        "# obtain evaluator.\r\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\r\n",
        "\r\n",
        "# compute the classification error on test data.\r\n",
        "accuracy = evaluator.evaluate(predictions)\r\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5cf2411ee50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# generate the train/test split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "bkmx0l2AxFbZ",
        "outputId": "4309c256-fdea-42a9-acc6-142351ee8545"
      },
      "source": [
        "evaluator.evaluate(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-e66e76d20ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: prediction does not exist. Available: label, features"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2jltkgtkNla",
        "outputId": "4528e2fa-f3d6-407d-b19b-04f8971c39bc"
      },
      "source": [
        "predict.accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11236666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMztZ2eOOrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852c8f8a-693b-44b9-afcc-fec9f6b984ba"
      },
      "source": [
        "id='1kUIrskM0zNH8u71G9M1BkHjRQYxvgAvh'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('data.zip')\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/regions.csv        \n",
            "  inflating: data/departments.csv    \n",
            "  inflating: data/jobs.csv           \n",
            "  inflating: data/locations.csv      \n",
            "  inflating: data/country.csv        \n",
            "  inflating: data/employees.csv      \n",
            "  inflating: data/job_history.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "xFSdAgFORztn",
        "outputId": "46c9088d-4d63-433b-c6b5-a1e659610257"
      },
      "source": [
        "regions = spark.read.csv('data/regions.csv',sep = '\\t',header = True)\r\n",
        "regions.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cbe4dfbb4a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/regions.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mregions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "q2dYL3K6Rz1b",
        "outputId": "f06ea761-b690-426e-860d-67e0c25c41ab"
      },
      "source": [
        "departments = spark.read.csv('data/departments.csv',sep = '\\t',header = True)\r\n",
        "departments.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-46ea1d0146c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdepartments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/departments.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdepartments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W426QX1YRt6U",
        "outputId": "2732172d-c026-4f5b-bc0e-c7774b47716b"
      },
      "source": [
        "jobs = spark.read.csv('data/jobs.csv',sep = '\\t',header = True)\r\n",
        "jobs.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+----------+----------+\n",
            "|    JOB_ID|           JOB_TITLE|MIN_SALARY|MAX_SALARY|\n",
            "+----------+--------------------+----------+----------+\n",
            "|   AD_PRES|           President|     20080|     40000|\n",
            "|     AD_VP|Administration Vi...|     15000|     30000|\n",
            "|   AD_ASST|Administration As...|      3000|      6000|\n",
            "|    FI_MGR|     Finance Manager|      8200|     16000|\n",
            "|FI_ACCOUNT|          Accountant|      4200|      9000|\n",
            "+----------+--------------------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoiIbRhARuGE",
        "outputId": "6b306209-eb55-4e72-ab78-4d31e21d7788"
      },
      "source": [
        "locations = spark.read.csv('data/locations.csv',sep = '\\t',header = True)\r\n",
        "locations.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "|LOCATION_ID|      STREET_ADDRESS|POSTAL_CODE|     CITY|  STATE_PROVINCE|COUNTRY_ID|\n",
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "|       1000|1297 Via Cola di Rie|      00989|     Roma|            null|        IT|\n",
            "|       1100|93091 Calle della...|      10934|   Venice|            null|        IT|\n",
            "|       1200|    2017 Shinjuku-ku|       1689|    Tokyo|Tokyo Prefecture|        JP|\n",
            "|       1300|     9450 Kamiya-cho|       6823|Hiroshima|            null|        JP|\n",
            "|       1400| 2014 Jabberwocky Rd|      26192|Southlake|           Texas|        US|\n",
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9jbcANwSXZg",
        "outputId": "adc00d59-2c45-4c74-fa56-83161a818e69"
      },
      "source": [
        "country = spark.read.csv('data/country.csv',sep = '\\t',header = True)\r\n",
        "country.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+---------+\n",
            "|COUNTRY_ID|COUNTRY_NAME|REGION_ID|\n",
            "+----------+------------+---------+\n",
            "|        AR|   Argentina|        2|\n",
            "|        AU|   Australia|        3|\n",
            "|        BE|     Belgium|        1|\n",
            "|        BR|      Brazil|        2|\n",
            "|        CA|      Canada|        2|\n",
            "+----------+------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ-xaItPSmO9",
        "outputId": "939850cd-cf8e-4d09-b345-00487e78313a"
      },
      "source": [
        "employees = spark.read.csv('data/employees.csv',sep = '\\t',header = True)\r\n",
        "employees.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|        100|    Steven|     King|   SKING|515.123.4567| 17.06.03|AD_PRES| 24000|          null|      null|           90|\n",
            "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568| 21.09.05|  AD_VP| 17000|          null|       100|           90|\n",
            "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569| 13.01.01|  AD_VP| 17000|          null|       100|           90|\n",
            "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567| 03.01.06|IT_PROG|  9000|          null|       102|           60|\n",
            "|        104|     Bruce|    Ernst|  BERNST|590.423.4568| 21.05.07|IT_PROG|  6000|          null|       103|           60|\n",
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTlUH66fSmfc",
        "outputId": "82939334-4ca7-4a78-bcc8-113630172482"
      },
      "source": [
        "job_history = spark.read.csv('data/job_history.csv',sep = '\\t',header = True)\r\n",
        "job_history.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+--------+----------+-------------+\n",
            "|EMPLOYEE_ID|START_DATE|END_DATE|    JOB_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+--------+----------+-------------+\n",
            "|        102|  13.01.01|24.07.06|   IT_PROG|           60|\n",
            "|        101|  21.09.97|27.10.01|AC_ACCOUNT|          110|\n",
            "|        101|  28.10.01|15.03.05|    AC_MGR|          110|\n",
            "|        201|  17.02.04|19.12.07|    MK_REP|           20|\n",
            "|        114|  24.03.06|31.12.07|  ST_CLERK|           50|\n",
            "+-----------+----------+--------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v43K2fDAjUYb"
      },
      "source": [
        "Кто получает больше всего? Кто меньше всего?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv0gpyu-zVs5",
        "outputId": "4b3684a7-473e-4129-e97c-1fe96ec0972d"
      },
      "source": [
        "employees_new = employees.withColumn(\"SALARY\", col(\"SALARY\").cast(\"int\"))\r\n",
        "min_val = employees_new.agg(min(col(\"SALARY\"))).collect()[0][0]\r\n",
        "max_val = employees_new.agg(max(col(\"SALARY\"))).collect()[0][0]\r\n",
        "employees_new.filter(employees_new.SALARY==min_val).show()\r\n",
        "employees_new.filter(employees_new.SALARY==max_val).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+-------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|  EMAIL|PHONE_NUMBER|HIRE_DATE|  JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+-------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "|        132|        TJ|    Olson|TJOLSON|650.124.8234| 10.04.07|ST_CLERK|  2100|          null|       121|           50|\n",
            "+-----------+----------+---------+-------+------------+---------+--------+------+--------------+----------+-------------+\n",
            "\n",
            "+-----------+----------+---------+-----+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+-----+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|        100|    Steven|     King|SKING|515.123.4567| 17.06.03|AD_PRES| 24000|          null|      null|           90|\n",
            "+-----------+----------+---------+-----+------------+---------+-------+------+--------------+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg8-rkFlkMqm"
      },
      "source": [
        "Выведете топ 5 по зарплате."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMzJ_tJhnWuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620b879d-a1de-41a9-de80-06fc4552c485"
      },
      "source": [
        "employees_new.sort(\"SALARY\", ascending=False).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+--------+------------------+---------+-------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|      PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+--------+------------------+---------+-------+------+--------------+----------+-------------+\n",
            "|        100|    Steven|     King|   SKING|      515.123.4567| 17.06.03|AD_PRES| 24000|          null|      null|           90|\n",
            "|        101|     Neena|  Kochhar|NKOCHHAR|      515.123.4568| 21.09.05|  AD_VP| 17000|          null|       100|           90|\n",
            "|        102|       Lex|  De Haan| LDEHAAN|      515.123.4569| 13.01.01|  AD_VP| 17000|          null|       100|           90|\n",
            "|        145|      John|  Russell| JRUSSEL|011.44.1344.429268| 01.10.04| SA_MAN| 14000|           0,4|       100|           80|\n",
            "|        146|     Karen| Partners|KPARTNER|011.44.1344.467268| 05.01.05| SA_MAN| 13500|           0,3|       100|           80|\n",
            "+-----------+----------+---------+--------+------------------+---------+-------+------+--------------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfBpDVYwkM7K"
      },
      "source": [
        "Сколько всего регионов? Сколько работников в каждом регионе?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frEFGpMmjNBQ"
      },
      "source": [
        "regions.createOrReplaceTempView(\"regions_sql\") \r\n",
        "departments.createOrReplaceTempView(\"departments_sql\") \r\n",
        "jobs.createOrReplaceTempView(\"jobs_sql\") \r\n",
        "locations.createOrReplaceTempView(\"locations_sql\") \r\n",
        "country.createOrReplaceTempView(\"country_sql\") \r\n",
        "employees.createOrReplaceTempView(\"employees_sql\") \r\n",
        "job_history.createOrReplaceTempView(\"job_history_sql\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgqNuilWAV6z",
        "outputId": "7188dba3-2707-425f-cb0d-336b67a57068"
      },
      "source": [
        "regions.select(\"REGION_NAME\").distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2d2AiBknYSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59097e5b-b0fc-4ed2-812d-b46c42c04831"
      },
      "source": [
        "query = spark.sql(\r\n",
        "    '''\r\n",
        "        SELECT REGION_NAME, COUNT(REGION_NAME)\r\n",
        "        \r\n",
        "        FROM employees_sql \r\n",
        "\r\n",
        "          JOIN departments_sql\r\n",
        "            ON employees_sql.DEPARTMENT_ID = departments_sql.DEPARTMENT_ID\r\n",
        "\r\n",
        "          JOIN locations_sql\r\n",
        "            ON departments_sql.LOCATION_ID = locations_sql.LOCATION_ID\r\n",
        "\r\n",
        "          JOIN country_sql\r\n",
        "            ON locations_sql.COUNTRY_ID = country_sql.COUNTRY_ID\r\n",
        "\r\n",
        "          JOIN regions_sql\r\n",
        "            ON country_sql.REGION_ID = regions_sql.REGION_ID\r\n",
        "       \r\n",
        "        GROUP BY REGION_NAME\r\n",
        "    '''\r\n",
        ")\r\n",
        "query.show()\r\n",
        "#gdf = query.groupBy(\"REGION_NAME\")\r\n",
        "#sorted(gdf.agg({\"*\": \"count\"}).collect())\r\n",
        "#ORDER BY regions_sql.REGION_NAME\r\n",
        "#SELECT FIRST_NAME, LAST_NAME, REGION_NAME  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------------------+\n",
            "|REGION_NAME|count(REGION_NAME)|\n",
            "+-----------+------------------+\n",
            "|     Europe|                36|\n",
            "|   Americas|                70|\n",
            "+-----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOY2unJ8kNXz"
      },
      "source": [
        "Выведете всех работников из Китая."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lhj9GAwnZgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14846d7a-4c82-4683-cb27-241b8530fad0"
      },
      "source": [
        "query = spark.sql(\r\n",
        "    '''\r\n",
        "        SELECT FIRST_NAME, LAST_NAME \r\n",
        "        FROM employees_sql \r\n",
        "\r\n",
        "          JOIN departments_sql\r\n",
        "            ON employees_sql.DEPARTMENT_ID = departments_sql.DEPARTMENT_ID\r\n",
        "\r\n",
        "          JOIN locations_sql\r\n",
        "            ON departments_sql.LOCATION_ID = locations_sql.LOCATION_ID\r\n",
        "\r\n",
        "          JOIN country_sql\r\n",
        "            ON locations_sql.COUNTRY_ID = country_sql.COUNTRY_ID\r\n",
        "\r\n",
        "        WHERE country_sql.COUNTRY_NAME = 'China'\r\n",
        "    '''\r\n",
        ")\r\n",
        "query.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+\n",
            "|FIRST_NAME|LAST_NAME|\n",
            "+----------+---------+\n",
            "+----------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Nc8F-6kNR7"
      },
      "source": [
        "Укажите самую высокооплачиваемою должность."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZNAohM-naSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e81ebc8-ee8e-4c62-cb15-e0a1bf5af9dd"
      },
      "source": [
        "jobs_new = jobs.withColumn(\"MIN_SALARY\", col(\"MIN_SALARY\").cast(\"int\")).withColumn(\"MAX_SALARY\", col(\"MAX_SALARY\").cast(\"int\"))\r\n",
        "max_val = jobs_new.agg(max(col(\"MAX_SALARY\"))).collect()[0][0]\r\n",
        "jobs_new.filter(jobs_new.MAX_SALARY==max_val).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+----------+----------+\n",
            "| JOB_ID|JOB_TITLE|MIN_SALARY|MAX_SALARY|\n",
            "+-------+---------+----------+----------+\n",
            "|AD_PRES|President|     20080|     40000|\n",
            "+-------+---------+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzenJwUnkNL8"
      },
      "source": [
        "Выведете всех работников связанных с ИТ. Выведете их менеджеров. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eogYiLjXna3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b713447d-277a-4f13-d461-0965447ac095"
      },
      "source": [
        "#Вывожу работников, в названии JOB_ID которых есть IT\r\n",
        "emp = employees.select(\r\n",
        "              \"EMPLOYEE_ID\", col(\"FIRST_NAME\").alias(\"FIRST_EMP_NAME\"), col(\"LAST_NAME\").alias(\"LAST_EMP_NAME\"), col(\"MANAGER_ID\").alias(\"ID\")\r\n",
        "              ).filter(employees.JOB_ID.contains('IT'))\r\n",
        "\r\n",
        "emp2 = employees.select(col(\"EMPLOYEE_ID\").alias(\"ID\"),col(\"FIRST_NAME\"),col(\"LAST_NAME\"))\r\n",
        "\r\n",
        "emp.join(emp2, emp.ID == emp2.ID).select(\r\n",
        "                                  \"FIRST_EMP_NAME\",\"LAST_EMP_NAME\",col(\"FIRST_NAME\").alias(\"FIRST_MAN_NAME\"),col(\"LAST_NAME\").alias(\"LAST_MAN_NAME\")\r\n",
        "                                    ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-------------+--------------+-------------+\n",
            "|FIRST_EMP_NAME|LAST_EMP_NAME|FIRST_MAN_NAME|LAST_MAN_NAME|\n",
            "+--------------+-------------+--------------+-------------+\n",
            "|     Alexander|       Hunold|           Lex|      De Haan|\n",
            "|         Bruce|        Ernst|     Alexander|       Hunold|\n",
            "|         David|       Austin|     Alexander|       Hunold|\n",
            "|         Valli|    Pataballa|     Alexander|       Hunold|\n",
            "|         Diana|      Lorentz|     Alexander|       Hunold|\n",
            "+--------------+-------------+--------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvpCoeYPmLTW"
      },
      "source": [
        "Выведете имя и фамилию работника, его текущую и предыдущую должности и сколько полных недель и дней прошло с момент изменения. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkCvyzkVnbo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81d6b6a-4fe4-414e-b124-e43dd05fafb9"
      },
      "source": [
        "#Интерпретирую вопрос как \"сколько полных дней и недель прошло с момента устройства на новую работу до текущего момента\"\r\n",
        "a = employees.alias('a')\r\n",
        "b = job_history.alias('b')\r\n",
        "b = b.withColumnRenamed(\"JOB_ID\", \"JOB_HIST_ID\")\r\n",
        "#меняем формат дат\r\n",
        "joined = a.join(b, a.EMPLOYEE_ID == b.EMPLOYEE_ID).select(\"FIRST_NAME\", \"LAST_NAME\", \"JOB_ID\", \"JOB_HIST_ID\", \"HIRE_DATE\", \"START_DATE\", \"END_DATE\")\r\n",
        "joined = joined.withColumn(\"HIRE_DATE\", to_date(col(\"HIRE_DATE\"),\"dd.MM.yy\"))\r\n",
        "joined = joined.withColumn(\"START_DATE\", to_date(col(\"START_DATE\"),\"dd.MM.yy\"))\r\n",
        "joined = joined.withColumn(\"END_DATE\", to_date(col(\"END_DATE\"),\"dd.MM.yy\"))\r\n",
        "#получилось плохо, 1997 год стал вдруг 2097. Если год выше 2050, то отнимем 100 от года\r\n",
        "joined = joined.withColumn(\"START_DATE\", when(year(col(\"START_DATE\")).cast('int')>2050, add_months(\"START_DATE\", -100*12)).otherwise(col(\"START_DATE\")))\r\n",
        "joined.show()\r\n",
        "#найдём строки, где действительно показаны предыдущие работы. Дата увольнения с предыдущей работы должна быть не позднее даты устройства на текущую\r\n",
        "joined = joined.filter(joined.END_DATE <= joined.HIRE_DATE)\r\n",
        "joined.show()\r\n",
        "#находим последнюю предыдущую (может быть несколько предыдущих работ)\r\n",
        "joined = joined.groupBy(\"FIRST_NAME\",\"LAST_NAME\",\"HIRE_DATE\").agg(max(col(\"END_DATE\")))\r\n",
        "joined.show()\r\n",
        "#считаем количество полных дней и недель с момента устройства на новую работу до текущего момента\r\n",
        "joined = joined.withColumn(\"days\", \r\n",
        "              datediff(to_date(lit(current_date())),\r\n",
        "                       to_date(\"HIRE_DATE\",\"dd.MM.yy\")))\r\n",
        "\r\n",
        "joined = joined.withColumn(\"weeks\", (col(\"days\").cast(\"int\")/7).cast(\"int\"))\r\n",
        "joined.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "|FIRST_NAME|LAST_NAME| JOB_ID|JOB_HIST_ID| HIRE_DATE|START_DATE|  END_DATE|\n",
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "|     Neena|  Kochhar|  AD_VP|     AC_MGR|2005-09-21|2001-10-28|2005-03-15|\n",
            "|     Neena|  Kochhar|  AD_VP| AC_ACCOUNT|2005-09-21|1997-09-21|2001-10-27|\n",
            "|       Lex|  De Haan|  AD_VP|    IT_PROG|2001-01-13|2001-01-13|2006-07-24|\n",
            "|       Den| Raphaely| PU_MAN|   ST_CLERK|2002-12-07|2006-03-24|2007-12-31|\n",
            "|     Payam| Kaufling| ST_MAN|   ST_CLERK|2003-05-01|2007-01-01|2007-12-31|\n",
            "|  Jonathon|   Taylor| SA_REP|     SA_MAN|2006-03-24|2007-01-01|2007-12-31|\n",
            "|  Jonathon|   Taylor| SA_REP|     SA_REP|2006-03-24|2006-03-24|2006-12-31|\n",
            "|  Jennifer|   Whalen|AD_ASST| AC_ACCOUNT|2003-09-17|2002-07-01|2006-12-31|\n",
            "|  Jennifer|   Whalen|AD_ASST|    AD_ASST|2003-09-17|1995-09-17|2001-06-17|\n",
            "|   Michael|Hartstein| MK_MAN|     MK_REP|2004-02-17|2004-02-17|2007-12-19|\n",
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "\n",
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "|FIRST_NAME|LAST_NAME| JOB_ID|JOB_HIST_ID| HIRE_DATE|START_DATE|  END_DATE|\n",
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "|     Neena|  Kochhar|  AD_VP|     AC_MGR|2005-09-21|2001-10-28|2005-03-15|\n",
            "|     Neena|  Kochhar|  AD_VP| AC_ACCOUNT|2005-09-21|1997-09-21|2001-10-27|\n",
            "|  Jennifer|   Whalen|AD_ASST|    AD_ASST|2003-09-17|1995-09-17|2001-06-17|\n",
            "+----------+---------+-------+-----------+----------+----------+----------+\n",
            "\n",
            "+----------+---------+----------+-------------+\n",
            "|FIRST_NAME|LAST_NAME| HIRE_DATE|max(END_DATE)|\n",
            "+----------+---------+----------+-------------+\n",
            "|     Neena|  Kochhar|2005-09-21|   2005-03-15|\n",
            "|  Jennifer|   Whalen|2003-09-17|   2001-06-17|\n",
            "+----------+---------+----------+-------------+\n",
            "\n",
            "+----------+---------+----------+-------------+----+-----+\n",
            "|FIRST_NAME|LAST_NAME| HIRE_DATE|max(END_DATE)|days|weeks|\n",
            "+----------+---------+----------+-------------+----+-----+\n",
            "|     Neena|  Kochhar|2005-09-21|   2005-03-15|5562|  794|\n",
            "|  Jennifer|   Whalen|2003-09-17|   2001-06-17|6297|  899|\n",
            "+----------+---------+----------+-------------+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBhGAaanC0e"
      },
      "source": [
        "Выведете уникальные телефонные номера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BrXyrsncKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f8d4b3-7351-4c34-de05-4567897de69a"
      },
      "source": [
        "employees_new.select(\"PHONE_NUMBER\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|      PHONE_NUMBER|\n",
            "+------------------+\n",
            "|011.44.1344.429018|\n",
            "|      515.127.4566|\n",
            "|      515.127.4564|\n",
            "|011.44.1344.429278|\n",
            "|      515.123.4569|\n",
            "|      650.124.1434|\n",
            "|      650.123.2234|\n",
            "|011.44.1344.498718|\n",
            "|      650.127.1634|\n",
            "|      515.127.4561|\n",
            "|011.44.1345.629268|\n",
            "|      515.127.4562|\n",
            "|011.44.1644.429264|\n",
            "|011.44.1644.429262|\n",
            "|      650.501.1876|\n",
            "|      650.127.1834|\n",
            "|011.44.1343.529268|\n",
            "|011.44.1644.429265|\n",
            "|      515.123.8181|\n",
            "|      650.507.9833|\n",
            "+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdf1VBv3nMR-"
      },
      "source": [
        "Есть ли сотрудники с одинаковыми фамилиями и сколько их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apTLX1o6jy6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c676c4b3-9100-4ae8-fc2f-2698272317e1"
      },
      "source": [
        "gdf = employees_new.groupBy(\"LAST_NAME\")\r\n",
        "count_list = sorted(gdf.agg({\"*\": \"count\"}).collect())\r\n",
        "count_df = spark.createDataFrame(count_list)\r\n",
        "count_df.filter(count_df[\"count(1)\"]>1).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    }
  ]
}